import streamlit as st
from PIL import Image
import sys
import os
import time

# --- ç¯å¢ƒè®¾ç½® ---
current_script_path = os.path.abspath(__file__)
pages_dir = os.path.dirname(current_script_path)
root_dir = os.path.dirname(pages_dir)
if root_dir not in sys.path:
    sys.path.append(root_dir)

try:
    import auth
    from services.llm_engine import LLMEngine
    from services.image_engine import ImageGenEngine
except ImportError as e:
    st.error(f"âŒ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    st.stop()

st.set_page_config(page_title="AI Studio", page_icon="ğŸ’¬", layout="wide")

# --- 1. åˆå§‹åŒ– ---
if 'auth' in sys.modules and not auth.check_password():
    st.stop()

# åˆå§‹åŒ–å¼•æ“
if "studio_ready" not in st.session_state:
    api_key = st.secrets.get("GOOGLE_API_KEY")
    st.session_state.llm_studio = LLMEngine(api_key)
    st.session_state.img_gen_studio = ImageGenEngine(api_key)
    st.session_state.studio_ready = True

# åˆå§‹åŒ–èŠå¤©å†å² [{"role": "user", "type": "text/image", "content": "..."}]
if "studio_msgs" not in st.session_state:
    st.session_state.studio_msgs = []

# åˆå§‹åŒ– Gemini Chat Session (ä»…ç”¨äºæ–‡æœ¬æ¨¡å‹)
if "gemini_chat" not in st.session_state:
    # é»˜è®¤ç”¨ Flash å¯åŠ¨
    model = st.session_state.llm_studio.get_chat_model("models/gemini-flash-latest")
    st.session_state.gemini_chat = model.start_chat(history=[])

# --- 2. ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.title("ğŸ›ï¸ AI å·¥ä½œå°")
    
    # === æ¨¡å‹é€‰æ‹© (æ ¸å¿ƒé€»è¾‘) ===
    # æŒ‰ç…§æ‚¨çš„è¦æ±‚æä¾›ä¸‰ä¸ªæ¨¡å‹
    model_map = {
        "âš¡ Gemini Flash (Fast Chat)": "models/gemini-flash-latest",
        "ğŸ§  Gemini 3 Pro (Reasoning)": "models/gemini-3-pro-preview", 
        "ğŸ¨ Gemini 3 Image (Generation)": "models/gemini-3-pro-image-preview" 
    }
    
    selected_label = st.selectbox("ğŸ¤– é€‰æ‹©æ¨¡å‹åŠŸèƒ½", list(model_map.keys()))
    current_model_id = model_map[selected_label]
    
    # åˆ¤æ–­å½“å‰æ˜¯å¦æ˜¯ç”Ÿå›¾æ¨¡å¼
    is_image_mode = "image-preview" in current_model_id

    st.divider()

    # === å‚æ•°é…ç½® (æ ¹æ®æ¨¡å¼å˜åŒ–) ===
    if is_image_mode:
        st.info("ğŸ¨ **ç”Ÿå›¾æ¨¡å¼å·²æ¿€æ´»**")
        st.caption("ç›´æ¥åœ¨å¯¹è¯æ¡†è¾“å…¥ Prompt å³å¯ç”Ÿå›¾ã€‚")
        ratio = st.selectbox("ç”»å¹…æ¯”ä¾‹", ["1:1 (Square)", "4:3", "16:9", "9:16"])
        style_seed = st.number_input("Seed (-1éšæœº)", value=-1)
    else:
        st.caption("ğŸ§  **ç³»ç»Ÿäººè®¾ (System Prompt)**")
        sys_prompt = st.text_area("å®šä¹‰AIè§’è‰²", value="ä½ æ˜¯ä¸€ä¸ªäºšé©¬é€Šç”µå•†ä¸“å®¶ã€‚", height=100)
        
    st.divider()
    
    # === è®°å¿†ç®¡ç† ===
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å² / æ–°è¯é¢˜", use_container_width=True):
        st.session_state.studio_msgs = []
        # é‡ç½® Chat Session
        if not is_image_mode:
            new_model = st.session_state.llm_studio.get_chat_model(current_model_id, sys_prompt)
            st.session_state.gemini_chat = new_model.start_chat(history=[])
        st.rerun()

# --- 3. ä¸»ç•Œé¢ ---
st.title("ğŸ’¬ Amazon AI Studio")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.studio_msgs:
    with st.chat_message(msg["role"]):
        # å¦‚æœæ˜¯å›¾ç‰‡ç±»å‹çš„æ¶ˆæ¯
        if msg.get("type") == "image_result":
            st.image(msg["content"], caption="Generated Image")
        # å¦‚æœæ˜¯åŒ…å«ä¸Šä¼ å›¾çš„ç”¨æˆ·æ¶ˆæ¯
        elif msg.get("ref_image"):
            st.image(msg["ref_image"], width=250)
            st.markdown(msg["content"])
        # æ™®é€šæ–‡æœ¬
        else:
            st.markdown(msg["content"])

# --- 4. è¾“å…¥å¤„ç† ---
# ä¸Šä¼ å›¾ç‰‡ç»„ä»¶ (ä»…æ–‡æœ¬æ¨¡å¼æ”¯æŒè¯†å›¾ï¼Œç”Ÿå›¾æ¨¡å¼æ”¯æŒå‚è€ƒå›¾)
uploaded_file = st.file_uploader("ğŸ“· ä¸Šä¼ å›¾ç‰‡ (è¯†å›¾/å‚è€ƒ)", type=["jpg", "png", "webp"], label_visibility="collapsed")

user_input = st.chat_input("è¾“å…¥æŒ‡ä»¤æˆ– Prompt...")

if user_input:
    # å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡
    input_image = None
    if uploaded_file:
        input_image = Image.open(uploaded_file)
    
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.session_state.studio_msgs.append({
        "role": "user", 
        "content": user_input,
        "ref_image": input_image
    })
    with st.chat_message("user"):
        if input_image: st.image(input_image, width=250)
        st.markdown(user_input)

    # 2. AI å“åº” (åˆ†æµé€»è¾‘)
    with st.chat_message("assistant"):
        
        # === åˆ†æ”¯ A: ç”Ÿå›¾æ¨¡å¼ ===
        if is_image_mode:
            with st.status("ğŸ¨ æ­£åœ¨ç»˜å›¾...", expanded=True) as status:
                try:
                    # è°ƒç”¨ Image Engine
                    img_bytes = st.session_state.img_gen_studio.generate(
                        prompt=user_input,
                        model_name=current_model_id,
                        ref_image=input_image, # æ”¯æŒå«å›¾
                        ratio_suffix=f", aspect ratio {ratio.split()[0]}",
                        seed=int(style_seed) if style_seed != -1 else None
                    )
                    
                    if img_bytes:
                        st.image(img_bytes, caption="Generated by Gemini 3 Image")
                        # ä¿å­˜åˆ°å†å²
                        st.session_state.studio_msgs.append({
                            "role": "assistant",
                            "type": "image_result",
                            "content": img_bytes
                        })
                        status.update(label="âœ… ç»˜å›¾å®Œæˆ", state="complete")
                    else:
                        st.error("ç”Ÿæˆå¤±è´¥ï¼Œå¯èƒ½è§¦å‘äº†å®‰å…¨æ‹¦æˆªã€‚")
                        status.update(label="âŒ ä»»åŠ¡ä¸­æ­¢", state="error")
                except Exception as e:
                    st.error(f"Error: {e}")

        # === åˆ†æ”¯ B: æ–‡æœ¬/å¯¹è¯æ¨¡å¼ ===
        else:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢ Session æ¨¡å‹ (å¦‚æœç”¨æˆ·åœ¨ä¸­é€”åˆ‡æ¢äº†ä¸‹æ‹‰æ¡†)
            # ç®€å•çš„åšæ³•ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ç”¨æˆ·åˆ‡æ¢æ¨¡å‹åç‚¹äº†æ¸…ç©ºï¼Œæˆ–è€…æˆ‘ä»¬åŠ¨æ€é‡è¿
            # ä¸ºäº†æµç•…ä½“éªŒï¼Œè¿™é‡ŒåŠ¨æ€è°ƒç”¨ chat_stream å³å¯
            
            stream_placeholder = st.empty()
            full_response = ""
            
            try:
                # é‡æ–°è·å–ä¸€æ¬¡å¸¦æœ€æ–° System Prompt çš„ Chat Session 
                # (æ³¨æ„ï¼šåœ¨é•¿å¯¹è¯ä¸­é¢‘ç¹åˆ‡æ¢ System Prompt å¯èƒ½ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡é”™ä¹±ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†)
                if not st.session_state.gemini_chat:
                     model = st.session_state.llm_studio.get_chat_model(current_model_id)
                     st.session_state.gemini_chat = model.start_chat(history=[])
                
                # å¼€å§‹æµå¼å¯¹è¯
                response_stream = st.session_state.llm_studio.chat_stream(
                    st.session_state.gemini_chat, 
                    user_input, 
                    input_image
                )
                
                for chunk in response_stream:
                    full_response += chunk
                    stream_placeholder.markdown(full_response + "â–Œ")
                
                stream_placeholder.markdown(full_response)
                
                # ä¿å­˜æ–‡æœ¬å†å²
                st.session_state.studio_msgs.append({
                    "role": "assistant",
                    "content": full_response
                })
                
            except Exception as e:
                st.error(f"å¯¹è¯å¼‚å¸¸: {e}")
