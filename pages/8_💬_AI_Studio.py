import streamlit as st
from PIL import Image
import sys
import os

# ç¯å¢ƒè®¾ç½® (ä¸ä¹‹å‰ä¸€è‡´)
current_script_path = os.path.abspath(__file__)
pages_dir = os.path.dirname(current_script_path)
root_dir = os.path.dirname(pages_dir)
if root_dir not in sys.path:
    sys.path.append(root_dir)

try:
    import auth
    from services.llm_engine import LLMEngine
    from services.image_engine import ImageGenEngine # å¦‚æœä½ æƒ³åœ¨è¿™é‡Œä¹Ÿæ”¯æŒç”Ÿå›¾
except ImportError as e:
    st.error(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    st.stop()

st.set_page_config(page_title="Amazon AI Studio", page_icon="ğŸ’¬", layout="wide")

# --- 1. åˆå§‹åŒ– ---
if 'auth' in sys.modules and not auth.check_password():
    st.stop()

if "llm_studio" not in st.session_state:
    api_key = st.secrets.get("GOOGLE_API_KEY")
    st.session_state.llm_studio = LLMEngine(api_key)

# æ ¸å¿ƒï¼šç®¡ç†èŠå¤©å†å²å’Œä¼šè¯å¯¹è±¡
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = [] # ç”¨äºUIæ˜¾ç¤º [{"role": "user", "content": "hi", "image": img}, ...]

if "gemini_chat_session" not in st.session_state:
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ Gemini ä¼šè¯
    model = st.session_state.llm_studio.get_chat_model()
    st.session_state.gemini_chat_session = model.start_chat(history=[])

# --- 2. ä¾§è¾¹æ é…ç½® (æ§åˆ¶å°é£æ ¼) ---
with st.sidebar:
    st.title("ğŸ›ï¸ AI Studio æ§åˆ¶å°")
    
    # A. æ¨¡å‹é€‰æ‹©
    model_options = [
        "models/gemini-3-pro-preview", 
        "models/gemini-flash-latest",
        "models/gemini-flash-lite-latest"
    ]
    selected_model = st.selectbox("ğŸ¤– æ¨¡å‹é€‰æ‹©", model_options)
    
    # B. ç³»ç»ŸæŒ‡ä»¤ (System Prompt) - è¿™å°±æ˜¯"äººè®¾"
    st.caption("ğŸ§  ç³»ç»ŸæŒ‡ä»¤ (System Instructions)")
    system_prompt = st.text_area(
        "å®šä¹‰ AI çš„è¡Œä¸º", 
        value="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äºšé©¬é€Šç”µå•†è¿è¥ä¸“å®¶ã€‚å›ç­”è¦ç®€æ´ã€å•†ä¸šåŒ–ï¼Œå¹¶å–„äºåˆ†æäº§å“å–ç‚¹ã€‚",
        height=150,
        help="åœ¨è¿™é‡Œå‘Šè¯‰ AI å®ƒæ˜¯è°ï¼Œæ¯”å¦‚'ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ–‡æ¡ˆ'æˆ–'ä½ æ˜¯ä¸€ä¸ªPythonä»£ç åŠ©æ‰‹'ã€‚"
    )
    
    st.divider()
    
    # C. è®°å¿†ç®¡ç† (æ ¸å¿ƒéœ€æ±‚)
    col_mem1, col_mem2 = st.columns([1, 3])
    with col_mem1:
        st.write("") # Spacer
    with col_mem2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤è®°å¿† (Reset)", type="primary", use_container_width=True):
            # 1. æ¸…ç©º UI å†å²
            st.session_state.chat_messages = []
            # 2. é‡ç½® Gemini åç«¯ä¼šè¯
            new_model = st.session_state.llm_studio.get_chat_model(selected_model, system_prompt)
            st.session_state.gemini_chat_session = new_model.start_chat(history=[])
            st.toast("è®°å¿†å·²æ¸…é™¤ï¼Œå¼€å¯æ–°è¯é¢˜ï¼", icon="ğŸ§¹")
            st.rerun()

    st.info("ğŸ’¡ **æç¤º**: ä½ å¯ä»¥ç›´æ¥æˆªå›¾ç²˜è´´åˆ°å¯¹è¯æ¡†ï¼Œæˆ–è€…ç‚¹å‡»å›å½¢é’ˆä¸Šä¼ å›¾ç‰‡ã€‚")

# --- 3. ä¸»å¯¹è¯åŒº ---
st.title("ğŸ’¬ Amazon AI Workbench")
st.caption("ä¸ AI è‡ªç”±å¯¹è¯ï¼Œåˆ†æå›¾ç‰‡ã€æ’°å†™æ–‡æ¡ˆæˆ–æ„æ€åˆ›æ„ã€‚")

# å±•ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.chat_messages:
    with st.chat_message(msg["role"]):
        # å¦‚æœæœ‰å›¾ç‰‡å…ˆå±•ç¤ºå›¾ç‰‡
        if "image" in msg and msg["image"]:
            st.image(msg["image"], width=300)
        st.markdown(msg["content"])

# --- 4. è¾“å…¥å¤„ç† ---
# ä¸Šä¼ å›¾ç‰‡çš„å°æŒ‚ä»¶ (æ”¾åœ¨è¾“å…¥æ¡†ä¸Šæ–¹æˆ–ä¾§è¾¹æ¯”è¾ƒéš¾ï¼ŒStreamlité™åˆ¶ï¼Œé€šå¸¸ç”¨ expander æˆ– file_uploader)
with st.expander("ğŸ“· ä¸Šä¼ å›¾ç‰‡ (å¯é€‰)", expanded=False):
    uploaded_img = st.file_uploader("æ·»åŠ å›¾ç‰‡åˆ°å¯¹è¯", type=["png", "jpg", "webp", "jpeg"], label_visibility="collapsed")

prompt = st.chat_input("è¾“å…¥ä½ çš„æŒ‡ä»¤...")

if prompt:
    # 1. å¤„ç†ç”¨æˆ·è¾“å…¥
    user_img = None
    if uploaded_img:
        user_img = Image.open(uploaded_img)
    
    # æ›´æ–° UI å†å²
    st.session_state.chat_messages.append({
        "role": "user", 
        "content": prompt,
        "image": user_img
    })
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        if user_img:
            st.image(user_img, width=300)
        st.markdown(prompt)

    # 2. AI å›å¤
    with st.chat_message("assistant"):
        stream_placeholder = st.empty()
        full_response = ""
        
        # ç¡®ä¿æ¨¡å‹ä¸ä¾§è¾¹æ é…ç½®åŒæ­¥ (å¦‚æœç³»ç»ŸæŒ‡ä»¤å˜äº†ï¼Œå…¶å®åº”è¯¥é‡ç½® sessionï¼Œä½†åœ¨ç®€å•æ¨¡å¼ä¸‹æˆ‘ä»¬åªæ›´æ–° session å¯¹è±¡)
        # æ³¨æ„ï¼šåŠ¨æ€ä¿®æ”¹ System Prompt åœ¨è¿è¡Œä¸­çš„ Session æ¯”è¾ƒéº»çƒ¦ï¼Œé€šå¸¸å»ºè®®ä¿®æ”¹åç‚¹"æ¸…é™¤è®°å¿†"ç”Ÿæ•ˆ
        # è¿™é‡Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨ chat_stream
        
        try:
            # è·å–æµå¼ç”Ÿæˆå™¨
            response_stream = st.session_state.llm_studio.chat_stream(
                st.session_state.gemini_chat_session,
                prompt,
                user_img
            )
            
            for chunk in response_stream:
                full_response += chunk
                stream_placeholder.markdown(full_response + "â–Œ")
            
            stream_placeholder.markdown(full_response)
            
            # æ›´æ–° UI å†å²
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": full_response
            })
            
        except Exception as e:
            st.error(f"å¯¹è¯å‡ºé”™: {e}")
            if "429" in str(e):
                st.warning("è¯·æ±‚è¿‡å¿«ï¼Œè¯·ç¨åé‡è¯•ã€‚")
