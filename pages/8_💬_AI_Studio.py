import streamlit as st
from PIL import Image
import sys
import os
import google.generativeai as genai

# --- è·¯å¾„ç¯å¢ƒè®¾ç½® ---
current_script_path = os.path.abspath(__file__)
pages_dir = os.path.dirname(current_script_path)
root_dir = os.path.dirname(pages_dir)
if root_dir not in sys.path: sys.path.append(root_dir)

try:
    import auth
    from services.llm_engine import LLMEngine
    from services.image_engine import ImageGenEngine
    # å¼•å…¥æˆ‘ä»¬åˆšæ‰æ‹†åˆ†å‡ºæ¥çš„ UI ç»„ä»¶
    from app_utils.ui_components import inject_chat_css, render_chat_message
    from app_utils.image_processing import create_preview_thumbnail
except ImportError as e:
    st.error(f"âŒ æ¨¡å—ç¼ºå¤±: {e}")
    st.stop()

st.set_page_config(page_title="Amazon AI Studio", page_icon="ğŸ§ª", layout="wide")
inject_chat_css() # æ³¨å…¥æ ·å¼

# --- æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def build_gemini_history(msgs):
    """
    âœ… è§£å†³ä¸Šä¸‹æ–‡é—®é¢˜ï¼š
    å°† Session State ä¸­çš„æ¶ˆæ¯æ ¼å¼åŒ–ä¸º Gemini API éœ€è¦çš„ chat history æ ¼å¼ã€‚
    æ³¨æ„ï¼šä¸åŒ…å«æœ€åä¸€æ¡æ­£åœ¨å‘é€çš„æ¶ˆæ¯ã€‚
    """
    history = []
    for m in msgs:
        # åªå¤„ç†æ–‡æœ¬ç±»å‹çš„å†å²ï¼Œå¿½ç•¥çº¯ç”Ÿå›¾ç»“æœï¼ˆGemini èŠå¤©æ¨¡å‹çœ‹ä¸æ‡‚ç”Ÿå›¾ç»“æœçš„ bytesï¼‰
        if m["type"] == "text" or m.get("ref_images"):
            parts = []
            # 1. æ”¾å…¥å›¾ç‰‡ (å¦‚æœæœ‰)
            if m.get("ref_images"):
                parts.extend(m["ref_images"])
            # 2. æ”¾å…¥æ–‡æœ¬
            if m["content"]:
                parts.append(m["content"])
            
            if parts:
                history.append({"role": m["role"], "parts": parts})
    return history

def delete_msg_callback(idx):
    if 0 <= idx < len(st.session_state.studio_msgs):
        st.session_state.studio_msgs.pop(idx)
        st.rerun()

def regenerate_callback(idx):
    # åˆ é™¤è¿™ä¸€æ¡ AI å›å¤ï¼Œå¹¶è§¦å‘é‡æ–°æ¨ç†
    if st.session_state.studio_msgs[idx]["role"] == "model":
        st.session_state.studio_msgs.pop(idx)
        st.session_state.trigger_inference = True
        st.rerun()

# --- åˆå§‹åŒ–ä¸é‰´æƒ ---
if 'auth' in sys.modules and not auth.check_password(): st.stop()

# âœ… ä¿®å¤ç‚¹ï¼šç‹¬ç«‹æ£€æŸ¥æ¯ä¸ªå…³é”®å˜é‡ï¼Œé˜²æ­¢æ—§çŠ¶æ€å¯¼è‡´çš„ AttributeError
if "studio_msgs" not in st.session_state:
    st.session_state.studio_msgs = []

if "msg_uid" not in st.session_state:
    st.session_state.msg_uid = 0

if "uploader_key_id" not in st.session_state:
    st.session_state.uploader_key_id = 0

if "studio_ready" not in st.session_state:
    api_key = st.secrets.get("GOOGLE_API_KEY")
    st.session_state.llm_studio = LLMEngine(api_key)
    st.session_state.img_gen_studio = ImageGenEngine(api_key)
    st.session_state.studio_ready = True

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ§ª AI Workbench")
    model_map = {
        "ğŸ§  Gemini 3 Pro (Reasoning)": "models/gemini-3-pro-preview", 
        "âš¡ Gemini Flash (Fast)": "models/gemini-flash-latest",
        "ğŸ¨ Gemini 3 Image (Image Gen)": "models/gemini-3-pro-image-preview" 
    }
    selected_label = st.selectbox("Model", list(model_map.keys()))
    current_model_id = model_map[selected_label]
    is_image_mode = "image-preview" in current_model_id

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
        st.session_state.studio_msgs = []
        st.session_state.uploader_key_id += 1 # åŒæ—¶æ¸…ç©ºæ–‡ä»¶é€‰æ‹©å™¨
        st.rerun()

# --- æ¶ˆæ¯æ¸²æŸ“å¾ªç¯ ---
# ä½¿ç”¨æ‹†åˆ†åçš„ render_chat_messageï¼Œä¸»ä»£ç æå…¶æ¸…çˆ½
for idx, msg in enumerate(st.session_state.studio_msgs):
    render_chat_message(idx, msg, delete_msg_callback, regenerate_callback)

# --- æ¨ç†é€»è¾‘ (åç«¯) ---
if st.session_state.get("trigger_inference", False):
    st.session_state.trigger_inference = False
    
    # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰æ¶ˆæ¯ï¼Œé˜²æ­¢ç©ºæŒ‡é’ˆ
    if not st.session_state.studio_msgs:
        st.rerun()

    last_msg = st.session_state.studio_msgs[-1]
    
    if last_msg["role"] == "user":
        with st.chat_message("model"):
            # === A. ç”Ÿå›¾æ¨¡å¼ ===
            if is_image_mode:
                with st.status("ğŸ¨ æ­£åœ¨ç»˜å›¾...", expanded=True):
                    try:
                        ref_img = last_msg["ref_images"][0] if last_msg.get("ref_images") else None
                        hd_bytes = st.session_state.img_gen_studio.generate(
                            prompt=last_msg["content"],
                            model_name=current_model_id,
                            ref_image=ref_img
                        )
                        if hd_bytes:
                            thumb = create_preview_thumbnail(hd_bytes, 800)
                            st.session_state.studio_msgs.append({
                                "role": "model", "type": "image_result",
                                "content": thumb, "hd_data": hd_bytes, 
                                "id": st.session_state.msg_uid
                            })
                            st.rerun()
                        else:
                            st.error("ç”Ÿæˆè¢«æ‹¦æˆªæˆ–å¤±è´¥")
                    except Exception as e:
                        st.error(f"Error: {e}")

            # === B. å¯¹è¯æ¨¡å¼ (å¸¦è®°å¿†) ===
            else:
                placeholder = st.empty()
                full_resp = ""
                try:
                    # 1. æ„å»ºå†å² (ä¸åŒ…å«åˆšæ‰å‘çš„è¿™æ¡)
                    past_history = build_gemini_history(st.session_state.studio_msgs[:-1])
                    
                    # 2. å¯åŠ¨èŠå¤©ä¼šè¯ (ç›´æ¥è°ƒç”¨ SDKï¼Œæœ€ç¨³å¦¥)
                    chat_session = genai.GenerativeModel(current_model_id).start_chat(history=past_history)
                    
                    # 3. å‡†å¤‡å½“å‰æ¶ˆæ¯ Payload
                    current_payload = []
                    if last_msg.get("ref_images"): current_payload.extend(last_msg["ref_images"])
                    if last_msg["content"]: current_payload.append(last_msg["content"])
                    
                    # 4. å‘é€å¹¶æµå¼æ¥æ”¶
                    response = chat_session.send_message(current_payload, stream=True)
                    for chunk in response:
                        if chunk.text:
                            full_resp += chunk.text
                            placeholder.markdown(full_resp + "â–Œ")
                    placeholder.markdown(full_resp)
                    
                    # 5. ä¿å­˜è®°å¿†
                    st.session_state.msg_uid += 1
                    st.session_state.studio_msgs.append({
                        "role": "model", "type": "text", 
                        "content": full_resp, "id": st.session_state.msg_uid
                    })
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"å¯¹è¯å‡ºé”™: {e}")

# --- åº•éƒ¨è¾“å…¥åŒº ---
if not st.session_state.get("trigger_inference", False):
    
    # âœ… è§£å†³é™„ä»¶é‡å¤é—®é¢˜ï¼šåŠ¨æ€ Key
    upload_key = f"uploader_{st.session_state.uploader_key_id}"
    
    with st.popover("ğŸ“", use_container_width=False):
        uploaded_files = st.file_uploader(
            "æ·»åŠ å‚è€ƒå›¾", 
            type=["jpg", "png", "webp"], 
            accept_multiple_files=True,
            key=upload_key 
        )
        if uploaded_files:
            st.info(f"å·²é€‰æ‹© {len(uploaded_files)} å¼ å›¾ç‰‡ (å‘é€åå°†æ¸…é™¤)")

    user_input = st.chat_input("è¾“å…¥æŒ‡ä»¤...")

    if user_input:
        # 1. å¤„ç†å›¾ç‰‡
        img_list = []
        if uploaded_files:
            for uf in uploaded_files:
                img_list.append(Image.open(uf))
        
        # 2. å­˜å…¥æ¶ˆæ¯é˜Ÿåˆ—
        st.session_state.msg_uid += 1
        st.session_state.studio_msgs.append({
            "role": "user",
            "type": "text",
            "content": user_input,
            "ref_images": img_list, # å›¾ç‰‡åªç»‘å®šåœ¨è¿™ä¸€æ¡æ¶ˆæ¯ä¸Š
            "id": st.session_state.msg_uid
        })
        
        # 3. å¼ºåˆ¶é‡ç½®ä¸Šä¼ æ§ä»¶
        st.session_state.uploader_key_id += 1
        
        # 4. è§¦å‘æ¨ç†
        st.session_state.trigger_inference = True
        st.rerun()
