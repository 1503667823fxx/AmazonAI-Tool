import streamlit as st
from PIL import Image
import sys
import os

# --- è·¯å¾„ç¯å¢ƒè®¾ç½® ---
current_script_path = os.path.abspath(__file__)
pages_dir = os.path.dirname(current_script_path)
root_dir = os.path.dirname(pages_dir)
if root_dir not in sys.path: sys.path.append(root_dir)

try:
    import auth
    from services.image_engine import ImageGenEngine
    # âœ… å¼•å…¥æ–°çš„é€»è¾‘é“¾ç®¡ç†å™¨
    from app_utils.chat_manager import ChatSessionManager 
    from app_utils.ui_components import render_chat_message, inject_chat_css
    from app_utils.image_processing import create_preview_thumbnail
except ImportError as e:
    st.error(f"âŒ æ¨¡å—ç¼ºå¤±: {e}")
    st.stop()

st.set_page_config(page_title="Amazon AI Studio", page_icon="ğŸ§ª", layout="wide")

# 1. æ³¨å…¥ CSS (ä¿æŒæ‚¨ä¹‹å‰çš„æ ·å¼)
inject_chat_css()
st.markdown("""
<style>
    /* å¼ºåˆ¶ä¸Šä¼ æŒ‰é’®åœ¨å·¦ä¸‹è§’ */
    section.main [data-testid="stPopover"] {
        position: fixed !important; bottom: 25px !important; left: 20px !important; z-index: 99999 !important;
        width: 45px !important; height: 45px !important;
    }
    section.main [data-testid="stPopover"] > div > button {
        border-radius: 50% !important; width: 45px !important; height: 45px !important;
        background-color: #fff !important; box-shadow: 0 4px 10px rgba(0,0,0,0.1) !important;
    } 
</style>
""", unsafe_allow_html=True)

# --- Session åˆå§‹åŒ– ---
if 'auth' in sys.modules and not auth.check_password(): st.stop()

# åŸºç¡€çŠ¶æ€
if "studio_msgs" not in st.session_state: st.session_state.studio_msgs = []
if "msg_uid" not in st.session_state: st.session_state.msg_uid = 0
if "uploader_key_id" not in st.session_state: st.session_state.uploader_key_id = 0
if "system_prompt_val" not in st.session_state: 
    st.session_state.system_prompt_val = "You are a helpful AI assistant for Amazon E-commerce sellers. Analyze images and text professionally."

# API åˆå§‹åŒ–
api_key = st.secrets.get("GOOGLE_API_KEY")
if not api_key:
    st.error("è¯·é…ç½® GOOGLE_API_KEY")
    st.stop()

if "img_gen_studio" not in st.session_state:
    st.session_state.img_gen_studio = ImageGenEngine(api_key)

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ§ª AI Workbench")
    
    # A. æ¨¡å‹é€‰æ‹©
    model_map = {
        "ğŸ§  Gemini 3 Pro (Reasoning)": "models/gemini-3-pro-preview", 
        "âš¡ Gemini Flash (Fast)": "models/gemini-flash-latest",
        "ğŸ¨ Gemini 3 Image (Image Gen)": "models/gemini-3-pro-image-preview" 
    }
    selected_label = st.selectbox("Core Model", list(model_map.keys()))
    current_model_id = model_map[selected_label]
    is_image_mode = "image-preview" in current_model_id

    st.divider()

    # B. ç³»ç»Ÿè®¾å®š (System Prompt) - è¿™æ‰æ˜¯å¯¹è¯çš„çµé­‚
    if not is_image_mode:
        st.caption("ğŸ­ System Persona")
        new_sys_prompt = st.text_area(
            "System Instruction", 
            value=st.session_state.system_prompt_val,
            height=100,
            help="å®šä¹‰ AI çš„èº«ä»½ï¼Œä¾‹å¦‚ï¼š'ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ—¶å°šä¹°æ‰‹' æˆ– 'ä½ æ˜¯ä¸€ä¸ªPythonä»£ç ä¸“å®¶'ã€‚"
        )
        # ä¿å­˜ System Prompt å˜åŠ¨
        if new_sys_prompt != st.session_state.system_prompt_val:
            st.session_state.system_prompt_val = new_sys_prompt
            # System Prompt å˜äº†ï¼Œæœ€å¥½æ¸…ç©ºå†å²ï¼Œæˆ–è€…è®©ç”¨æˆ·çŸ¥é“ä¸Šä¸‹æ–‡å˜äº†
            st.toast("System Prompt Updated!", icon="ğŸ’¾")

    st.divider()
    
    # C. æ“ä½œåŒº
    col_k1, col_k2 = st.columns(2)
    with col_k1:
        if st.button("ğŸ§¹ Clear", use_container_width=True):
            st.session_state.studio_msgs = []
            st.session_state.uploader_key_id += 1 
            st.rerun()
    with col_k2:
        if st.button("â†©ï¸ Undo", use_container_width=True):
            if st.session_state.studio_msgs:
                st.session_state.studio_msgs.pop() # åˆ æ‰ Model å›å¤
                if st.session_state.studio_msgs and st.session_state.studio_msgs[-1]["role"] == "user":
                   st.session_state.studio_msgs.pop() # ä¹Ÿåˆ æ‰ User æé—®ï¼Œå½»åº•å›é€€ä¸€æ­¥
                st.rerun()

# --- æ¶ˆæ¯æ¸²æŸ“ ---
def delete_msg_callback(idx):
    if 0 <= idx < len(st.session_state.studio_msgs):
        st.session_state.studio_msgs.pop(idx)
        st.rerun()

def regenerate_callback(idx):
    # é‡æ–°ç”Ÿæˆé€»è¾‘ï¼šåˆ æ‰å½“å‰çš„ AI å›å¤ï¼Œè§¦å‘é‡æ–°æ¨ç†
    if st.session_state.studio_msgs[idx]["role"] == "model":
        st.session_state.studio_msgs.pop(idx)
        st.session_state.trigger_inference = True
        st.rerun()

if not st.session_state.studio_msgs:
    st.info("ğŸ‘‹ Ready via **Chat Manager**. Upload images or text to start.")
else:
    for idx, msg in enumerate(st.session_state.studio_msgs):
        render_chat_message(idx, msg, delete_msg_callback, regenerate_callback)

# --- æ ¸å¿ƒæ¨ç†é€»è¾‘ (The Logical Chain) ---
if st.session_state.get("trigger_inference", False):
    st.session_state.trigger_inference = False
    if not st.session_state.studio_msgs: st.rerun()

    last_msg = st.session_state.studio_msgs[-1]
    
    # åªæœ‰å½“æœ€åä¸€æ¡æ˜¯ç”¨æˆ·å‘çš„æ¶ˆæ¯æ—¶ï¼Œæ‰è§¦å‘ AI å›å¤
    if last_msg["role"] == "user":
        with st.chat_message("model"):
            
            # === åˆ†æ”¯ A: ç”Ÿå›¾æ¨¡å¼ (æ— ä¸Šä¸‹æ–‡é€»è¾‘ï¼Œå•æ¬¡ç”Ÿæˆ) ===
            if is_image_mode:
                with st.status("ğŸ¨ Rendering...", expanded=True):
                    try:
                        ref_img = last_msg["ref_images"][0] if last_msg.get("ref_images") else None
                        hd_bytes = st.session_state.img_gen_studio.generate(
                            prompt=last_msg["content"],
                            model_name=current_model_id,
                            ref_image=ref_img
                        )
                        if hd_bytes:
                            thumb = create_preview_thumbnail(hd_bytes, 800)
                            st.session_state.studio_msgs.append({
                                "role": "model", "type": "image_result",
                                "content": thumb, "hd_data": hd_bytes, 
                                "id": st.session_state.msg_uid
                            })
                            st.rerun()
                        else:
                            st.error("Generation Failed / Blocked.")
                    except Exception as e:
                        st.error(f"Error: {e}")

            # === åˆ†æ”¯ B: æ™ºèƒ½å¯¹è¯æ¨¡å¼ (è°ƒç”¨ Chat Manager) ===
            else:
                placeholder = st.empty()
                full_resp = ""
                
                try:
                    # 1. åˆå§‹åŒ–é€»è¾‘å¤§è„‘ (ä¼ å…¥ System Prompt)
                    chat_manager = ChatSessionManager(
                        model_name=current_model_id, 
                        api_key=api_key,
                        system_instruction=st.session_state.system_prompt_val
                    )
                    
                    # 2. æ„å»ºå†å²ä¸Šä¸‹æ–‡ (ä¸åŒ…å«å½“å‰çš„æœ€åä¸€æ¡)
                    # æ³¨æ„ï¼šæˆ‘ä»¬æŠŠé™¤æœ€åä¸€æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œäº¤ç»™ Manager å»æ¸…æ´—ã€åˆå¹¶
                    history_msgs = st.session_state.studio_msgs[:-1]
                    chat_session = chat_manager.start_chat_session(history_msgs)
                    
                    # 3. å‡†å¤‡å½“å‰å‘é€çš„å†…å®¹ (User Turn)
                    current_payload = []
                    # é™„ä»¶ (å›¾ç‰‡)
                    if last_msg.get("ref_images"): 
                        current_payload.extend(last_msg["ref_images"])
                    # æ–‡æœ¬
                    if last_msg["content"]: 
                        current_payload.append(last_msg["content"])
                    
                    # 4. å‘é€ç»™ Gemini
                    # stream=True è®©ä½“éªŒåƒçœŸå®å¯¹è¯ä¸€æ ·æµç•…
                    response = chat_session.send_message(current_payload, stream=True)
                    
                    for chunk in response:
                        if chunk.text:
                            full_resp += chunk.text
                            placeholder.markdown(full_resp + "â–Œ")
                    placeholder.markdown(full_resp)
                    
                    # 5. è®°å½•å›å¤
                    st.session_state.msg_uid += 1
                    st.session_state.studio_msgs.append({
                        "role": "model", "type": "text", 
                        "content": full_resp, "id": st.session_state.msg_uid
                    })
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"Logic Chain Error: {e}")
                    # è°ƒè¯•ç”¨ï¼šæ˜¾ç¤ºå…·ä½“çš„é”™è¯¯æ ˆ
                    # st.exception(e)

# --- åº•éƒ¨è¾“å…¥åŒº ---
if not st.session_state.get("trigger_inference", False):
    
    upload_key = f"uploader_{st.session_state.uploader_key_id}"
    
    # é™„ä»¶æŒ‰é’® (å·¦ä¸‹è§’)
    with st.popover("ğŸ“", use_container_width=False):
        uploaded_files = st.file_uploader(
            "Upload Context Images", 
            type=["jpg", "png", "webp"], 
            accept_multiple_files=True,
            key=upload_key
        )
        if uploaded_files:
            st.caption(f"{len(uploaded_files)} images selected")

    # è¾“å…¥æ¡†
    user_input = st.chat_input("Type your message...")

    if user_input:
        img_list = []
        if uploaded_files:
            for uf in uploaded_files:
                img_list.append(Image.open(uf))
        
        st.session_state.msg_uid += 1
        st.session_state.studio_msgs.append({
            "role": "user",
            "type": "text",
            "content": user_input,
            "ref_images": img_list,
            "id": st.session_state.msg_uid
        })
        
        st.session_state.uploader_key_id += 1
        st.session_state.trigger_inference = True
        st.rerun()
