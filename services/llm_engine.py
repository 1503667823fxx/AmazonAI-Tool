import streamlit as st
import google.generativeai as genai
from services.styles import PRESETS

class LLMEngine:
    def __init__(self, api_key=None):
        self.api_key = api_key or st.secrets.get("GOOGLE_API_KEY")
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.valid = True
        else:
            self.valid = False
            
    # --- æ¨¡å‹è·¯ç”±é…ç½® ---
    def _get_model(self, model_type="reasoning"):
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†é…æ‚¨æŒ‡å®šçš„æ¨¡å‹
        """
        if model_type == "reasoning":
            # ä¼˜å…ˆä½¿ç”¨æœ€èªæ˜çš„ Gemini 3 Pro Preview å¤„ç†å¤æ‚çš„ Prompt æ¨ç†
            return genai.GenerativeModel("models/gemini-3-pro-preview")
        elif model_type == "fast":
            # ç¿»è¯‘æˆ–ç®€å•ä»»åŠ¡ä½¿ç”¨ Flash Latest
            return genai.GenerativeModel("models/gemini-flash-latest")
        return genai.GenerativeModel("models/gemini-flash-lite-latest")

    def translate(self, text, target_lang="English"):
        if not text or not self.valid: return text
        try:
            model = self._get_model("fast")
            prompt = f"Translate the following text to {target_lang}. Return ONLY the translation, no extra text.\nText: {text}"
            resp = model.generate_content(prompt)
            return resp.text.strip()
        except: return text

    def optimize_art_director_prompt(self, user_idea, task_type, weight, style_key, image_input=None, enable_split=False):
        """
        ä¼˜åŒ–æ ¸å¿ƒï¼šè§£å†³'æ¢æ¨¡ç‰¹'æ— æ•ˆçš„é—®é¢˜ã€‚
        ç­–ç•¥ï¼šå¼ºåˆ¶ LLM æå–åŸå›¾çš„'æœè£…/ç¯å¢ƒ'ï¼Œä½†é‡å†™'äººç‰©ç‰¹å¾'ã€‚
        """
        if not self.valid: return []

        style_data = PRESETS.get(style_key, PRESETS["ğŸ’¡ é»˜è®¤ (None)"])
        style_desc = style_data["desc"]

        # æ„å»ºå¤šæ¨¡æ€è¾“å…¥
        inputs = []
        inputs.append(image_input if image_input else "No reference image provided.")
        
        # --- æ ¸å¿ƒ System Prompt ---
        # è¿™ä¸€æ®µ Prompt æ˜¯è§£å†³ Bug çš„å…³é”®
        system_prompt = f"""
        You are an expert AI Art Director. Your goal is to write a precise image generation prompt based on the User's Request and the Reference Image.

        ã€User Requestã€‘: "{user_idea}"
        ã€Style Presetã€‘: "{style_desc}"

        ã€CRITICAL INSTRUCTION FOR IDENTITY SWAPPINGã€‘
        Analyze if the user wants to CHANGE the model/person (e.g., "swap model", "use a foreigner", "change to man").
        
        IF YES (Change Model):
        1. **IGNORE** the face/body traits in the Reference Image.
        2. **INVENT** specific, high-contrast physical details for the new person to OVERRIDE the image signal.
           - Instead of just "Western model", write: "Portrait of a Caucasian female model, platinum blonde wavy hair, icy blue eyes, fair skin structure, sharp jawline."
           - Instead of just "Black model", write: "Portrait of an African American male model, dark skin tone, short buzz cut, brown eyes."
        3. **KEEP** the clothing details from the Reference Image (describe the clothes you see in the image explicitly).

        IF NO (Keep Model):
        1. Describe the person in the Reference Image accurately to maintain consistency.

        ã€Final Output Formatã€‘
        Write a single, high-quality English prompt suitable for a text-to-image model.
        Format: [Subject Description (Face/Body)] + [Clothing Details (from Ref Image)] + [Action/Pose] + [Background/Environment] + [Lighting/Style].
        """
        
        inputs.append(system_prompt)

        try:
            # ä½¿ç”¨æœ€å¼ºçš„ Gemini 3 Pro Preview è¿›è¡Œæ€è€ƒ
            model = self._get_model("reasoning")
            
            config = genai.types.GenerationConfig(
                temperature=0.4, # é™ä½éšæœºæ€§ï¼Œç¡®ä¿ä¸¥æ ¼éµå¾ªæŒ‡ä»¤
                candidate_count=1
            )
            
            response = model.generate_content(inputs, generation_config=config)
            final_prompt = response.text.strip()
            
            return [final_prompt]

        except Exception as e:
            print(f"LLM Error: {e}")
            # é™çº§ç­–ç•¥
            return [f"{user_idea}, {style_desc}, high quality, 8k resolution"]
