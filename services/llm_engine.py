import streamlit as st
import google.generativeai as genai
from services.styles import PRESETS

class LLMEngine:
    def __init__(self, api_key=None):
        self.api_key = api_key or st.secrets.get("GOOGLE_API_KEY")
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.valid = True
        else:
            self.valid = False
            
    # --- æ¨¡å‹è·¯ç”±é…ç½® ---
    def _get_model(self, model_type="reasoning"):
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†é…æ‚¨æŒ‡å®šçš„æ¨¡å‹
        """
        if model_type == "reasoning":
            # ä¼˜å…ˆä½¿ç”¨æœ€èªæ˜çš„ Gemini 3 Pro Preview å¤„ç†å¤æ‚çš„ Prompt æ¨ç†
            return genai.GenerativeModel("models/gemini-3-pro-preview")
        elif model_type == "fast":
            # ç¿»è¯‘æˆ–ç®€å•ä»»åŠ¡ä½¿ç”¨ Flash Latest
            return genai.GenerativeModel("models/gemini-flash-latest")
        return genai.GenerativeModel("models/gemini-flash-lite-latest")

    def translate(self, text, target_lang="English"):
        if not text or not self.valid: return text
        try:
            model = self._get_model("fast")
            prompt = f"Translate the following text to {target_lang}. Return ONLY the translation, no extra text.\nText: {text}"
            resp = model.generate_content(prompt)
            return resp.text.strip()
        except: return text

def optimize_art_director_prompt(self, user_idea, task_type, weight, style_key, image_input=None, enable_split=False):
        """
        V6 é€»è¾‘å‡çº§ï¼šæ”¯æŒå¤šä¸»ä½“ç”Ÿæˆ (Multi-Subject Generation)
        è§£å†³ç—›ç‚¹ï¼šç”¨æˆ·è¦æ±‚"ç”Ÿæˆä¸¤ä½ä¸åŒæ¨¡ç‰¹"æ—¶ï¼ŒAI åªè¾“å‡ºå•äºº Promptã€‚
        """
        if not self.valid: return []

        style_data = PRESETS.get(style_key, PRESETS["ğŸ’¡ é»˜è®¤ (None)"])
        style_desc = style_data["desc"]

        # æ„å»ºè¾“å…¥
        inputs = []
        inputs.append(image_input if image_input else "No reference image provided.")
        
        # --- æ ¸å¿ƒ System Prompt (é’ˆå¯¹äººæ•°é—®é¢˜è¿›è¡Œäº†æ·±åº¦é‡æ„) ---
        system_prompt = f"""
        You are an expert AI Art Director. Your goal is to write a precise image generation prompt based on the User's Request and the Reference Image.

        ã€User Requestã€‘: "{user_idea}"
        ã€Style Presetã€‘: "{style_desc}"

        ã€STEP 1: ANALYZE SUBJECT COUNTã€‘
        Check if the user wants **MORE THAN ONE** person (e.g., "two models", "couple", "group", "twins", "friends").
        
        ğŸ‘‰ CASE A: MULTIPLE SUBJECTS (Target > 1 person)
        1. **Composition**: Start with "A medium shot of TWO models..." (or relevant number).
        2. **Differentiation**: You MUST invent DISTINCT looks for each model if requested.
           - Write: "Model on left is [Physique A, Hair A, Ethnicity A]. Model on right is [Physique B, Hair B, Ethnicity B]."
           - Do NOT make them look like clones unless user asks for "twins".
        3. **Clothing Logic**: Explicitly state that **BOTH** are wearing the clothing from the reference image (or as user requested).
           - Write: "Both models are wearing matching [Clothing Description from Ref Image]."

        ğŸ‘‰ CASE B: SINGLE SUBJECT (Target = 1 person)
        1. **Identity Check**: Does user want to change the model?
           - IF YES: Invent NEW physical traits (e.g., "Caucasian, blonde" or "Asian, short hair") to override the reference image face.
           - IF NO: Describe the person in the reference image accurately.

        ã€STEP 2: EXTRACT VISUALS FROM REFERENCEã€‘
        - Look at the Reference Image. Extract the **Clothing Details** (Texture, Color, Cut) and **Environment** (if needed).
        - If the user wants to keep the clothing, describe it in high detail so the generated image matches the product.

        ã€Final Output Formatã€‘
        Write a single, continuous English prompt.
        Structure: [Subject Count & Composition] + [Distinct Subject Details (Model A, Model B...)] + [Clothing/Product Details] + [Action/Interaction] + [Background] + [Style tags].
        """
        
        inputs.append(system_prompt)

        try:
            # ä¾ç„¶ä½¿ç”¨æœ€èªæ˜çš„ Gemini 3 Pro Preview (Reasoning)
            model = self._get_model("reasoning")
            
            config = genai.types.GenerationConfig(
                temperature=0.45, #ç¨å¾®æé«˜ä¸€ç‚¹ç‚¹åˆ›é€ åŠ›ï¼Œè®©å®ƒèƒ½ç¼–é€ å‡ºä¸¤ä¸ªä¸åŒçš„äºº
                candidate_count=1
            )
            
            response = model.generate_content(inputs, generation_config=config)
            final_prompt = response.text.strip()
            
            # è°ƒè¯•æ—¥å¿—ï¼šå¯ä»¥åœ¨åå°çœ‹åˆ° LLM åˆ°åº•è¾“å‡ºäº†ä»€ä¹ˆ
            print(f"ğŸ› Generated Prompt: {final_prompt}")
            
            return [final_prompt]

        except Exception as e:
            print(f"LLM Error: {e}")
            return [f"{user_idea}, {style_desc}, high quality, 8k resolution"]
